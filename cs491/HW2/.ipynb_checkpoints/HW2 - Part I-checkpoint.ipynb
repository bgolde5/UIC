{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary - Read the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kshemkalyani, Ajay  - parallel computation: models, algorithms, limits|Filtering and Prediction of Hidden Markov Models\n",
      "\n",
      "Sylvia Wolak  - Descartes' World|American Fiction & Mass Culture\n",
      "\n",
      "Ali Tafti  - Managerial Decision Making|The Design & Analysis of Trading Agents|Topics in Data Science\n",
      "\n",
      "Andy Johnson  - Cell & Molecular Biology|Computer Aided Visualization and Design\n",
      "\n",
      "Vincent Adiutori  - American Fiction & Mass Culture\n",
      "\n",
      "Alex Furman  - Probabilistic Graphical Models|Distributed Computing through Combinatorial Topology\n",
      "\n",
      "John Bell  - software engineering|Photo-Lab: The Visual Performance of Rights|Art After India\n",
      "\n",
      "Elise Archias  - Collections and Visual Knowledge in Early Modern Europe|Intro. to Modernism: Past, Future, Exile, Home\n",
      "\n",
      "Craig D. Foster  - Mechanics of Solids|Analytic Geometry & Calculus\n",
      "\n",
      "Lu, V Hui  - innovation and technology management ii|Models of Computation|Biomaterials|Pattern Recognition and Machine Learning|Computational Fluid Dynamics|Writing and Speaking Chineze I\n",
      "\n",
      "Ouri Wolfson  - Data-Driven Vizion and Graphics|Database Management Systems\n",
      "\n",
      "DasGupta, Bhaskar  - parallel computation: modelz, algorithms, limits\n",
      "\n",
      "Tanya Y Bergerwolf  - Recent Applications of Probability & Statistics|Algorithms for Big Data\n",
      "\n",
      "Lucia Valbonesi  - computer vizion\n",
      "\n",
      "Henri Gillet  - Data-Driven Vision and Graphics|Algebraic Geometry\n",
      "\n",
      "Matthew Siber  - reading & research|3D Photography\n",
      "\n",
      "Gallik, Kristin  - Molecular Genetics|Intro. to Computer Graphicz\n",
      "\n",
      "Hui Lu  - The Literary Scholar\n",
      "\n",
      "Kevin Brennan  - intermediate 3d computer animation|Astrophysics and Cosmology\n",
      "\n",
      "Ziebart, Brian  - machine learning reading group|Intro. to Number Theory\n",
      "\n",
      "William E. Walden  - Topics in Ecology and Evolutionary Biology|Back to the Future: Nostalgia & Futurity in Contemporary Sci-Fi TV & Telefantasy\n",
      "\n",
      "Amy Bailey  - Introduction to Science and Society: Theories and Controversies\n",
      "\n",
      "Julia Fish  - 3D Photography & Geometry Processing|The Foundation of Living Systems|Intro. to Computation for the Humanities and Social Sciences|Intro. to Computer Graphicz\n",
      "\n",
      "Clarno, Andy  - Chemical Process Design|Intro. to Computation for the Humanities & Social Sciences\n",
      "\n",
      "Erik Jones  - Partial Differential Equationz|Building Intelligent Robots|The Literary Scholar\n",
      "\n",
      "Milan Velebit  - Transport and Biotransport Processes|Reinventionz of Life: Aesthetics, Biopolitics, and the Avant-Gardes|Autonomous Agents and Computational Market Design|Advanced Programming for Digital Art and Literature\n",
      "\n",
      "Bailey  - operating systems\n",
      "\n",
      "Yanky Lekili  - Introductory Calculus, Part I|Filtering & Prediction of Hidden Markov Models\n",
      "\n",
      "Bassiri, Hormoz  - Principles of Ecology\n",
      "\n",
      "Tanya.Bergerwolf  - writing & speaking chinese i|Biomaterialz|Advanced Probabilistic Methods in Computer Science|Introduction to Filmmaking: Time & Form|Topics in Information Retrieval and Web Search|Architecture of the House Through Space and Time\n",
      "\n",
      "Eriksson, Jakob  - intro to computation for the humanities and social sciences|Computer Graphics Lab\n",
      "\n",
      "Anjum Ansari  - Foundationz of Electromagnetism and Modern Physics|Structural Analysis|American Fiction and Mass Culture\n",
      "\n",
      "Steve Sauerwald  - The Postcolonial and the Postmodern|Topics in Information Retrieval & Web Search|Computer Vision|Talking with Computers\n",
      "\n",
      "Sara D. Dunn  - introduction to architectural design\n",
      "\n",
      "William Walden  - special topicz in advanced algorithms|Environmental History|Introduction to Systems Programming|Topics in 3D Game Engine Development|Human Factors and User Interface Design\n",
      "\n",
      "Paolo Vinella  - topics in computer vision|Software System Design|Intro to Combinatorial Optimization\n",
      "\n",
      "Tanya Bergerwolf  - building a web application|Computational Methodz for Biology\n",
      "\n",
      "Mimi Dai  - introductory calculus, part ii\n",
      "\n",
      "Krall, Aaron  - nature and law in american literature\n",
      "\n",
      "Leon Fink  - the korean war in color|The Public Intellectual\n",
      "\n",
      "John Lillis  - Coding the Matrix: Linear Algebra through Computer Science Applicationz|Parallel and Distributed Programming|Intro to Computation for the Humanities & Social Sciences|Educational Software Seminar|Advanced Programming for Digital Art and Literature\n",
      "\n",
      "Will Walden  - Hilbert Spaces & Their Applications|Interdisciplinary Scientific Visualization\n",
      "\n",
      "Sistla, Prasad  - Intro. to Power Engineering|Database Management Systems|Dynamics & Vibrations\n",
      "\n",
      "Savar, Nina  - fieldwork in the urban community|Computational Topology|Intellectual Life & Culture in the Post-Western World\n",
      "\n",
      "Robert Kenyon  - Human Factors & User Interface Design\n",
      "\n",
      "Alexander Chudnovsky  - Fracture Mechanics|Intro. to Computation for the Humanities & Social Sciences|Foundations of Electromagnetism and Modern Physics|Topics in Grounded Language for Robotics\n",
      "\n",
      "Chris Kanich  - Computer Networks|Nineteenth-Century British Novel|Accelerated Introduction to Computer Science\n",
      "\n",
      "Mitch Theys  - Principles of Ecology|Introduction to Algorithms & Data Structures\n",
      "\n",
      "Lyons, Leilah  - cultures and countercultures: the american novel after world war ii|Human-Computer Interaction Seminar\n",
      "\n",
      "Troy, A. Patrick  - Computer Aided Visualization and Design\n",
      "\n",
      "Boy, Ugo  - hilbert spaces and their applications|Introduction to Filmmaking: Time and Form|Narrative and Immersion\n",
      "\n",
      "Cynthia Taylor  - biophysical & bioinorganic chemistry|The Aesthetics of Color: History, Theory, Critique\n",
      "\n",
      "Balajee Vamanan  - Data-Driven Vizion and Graphics|Complex Function Theory|Dynamics and Vibrations\n",
      "\n",
      "Aaron Krall  - nineteenth-century britizh novel|The Postcolonial and the Postmodern\n",
      "\n",
      "Joseph Hummel  - Transport and Biotransport Processes|Introduction to Algorithms and Data Structures|Introductory Calculuz, Part II|Introductory Compiler Construction\n",
      "\n",
      "Douglas Hogan  - operations research: probabilistic models|Computational Theory of Molecular Evolution|Interdisciplinary Scientific Visualization\n",
      "\n",
      "Didem Ozevin  - transport & biotransport processes|Honors Linear Algebra|Intro. to Scientific Computing & Problem Solving|Fundamentals of Computer Systems\n",
      "\n",
      "Robert Sloan  - introduction to computational linguistics\n",
      "\n",
      "Cody Cranch  - 3D Photography & Geometry Processing\n",
      "\n",
      "Gillet, Henri  - analytic geometry and calculus\n",
      "\n",
      "Lenore Zuck  - statistical inference ii|Basic Physics\n",
      "\n",
      "Jon Solworth  - Mechanics of Solids|Nineteenth-Century Architecture|Intro. to Computation for the Humanities and Social Sciences\n",
      "\n",
      "Eric Jones  - Filtering and Prediction of Hidden Markov Models\n",
      "\n",
      "Elisabeta Marai  - 2d game engines|Temporalitiez\n",
      "\n",
      "Ali O. Tafti  - Recent Applications of Probability & Statistics|Introduction to Scientific Computing\n",
      "\n",
      "Di Eugenio, Barbara  - the united states metropolis|Computational Physics|Fracture Mechanics\n",
      "\n",
      "Reed, Dale  - Thermodynamics|Introduction to Discrete Structures & Probability\n",
      "\n",
      "Vahe Caliskan  - electrical circuits & signals|Analytic Geometry & Calculus|Melville, Conrad, & the Sea|Statistical Mechanics\n",
      "\n",
      "Shonfeld, Dan  - Heat & Mass Transfer|Computer Aided Visualization & Design|Medical Bioinformatics\n",
      "\n",
      "Matthew D Siber  - Cell & Molecular Biology\n",
      "\n",
      "Philip Yu  - Special Topics in Advanced Algorithms\n",
      "\n",
      "Forbes, Angus  - Introductory Biochemistry\n",
      "\n",
      "Stephen Checkoway  - getting emotional: passionate theories|Topics in Ecology and Evolutionary Biology|Computer Systems Security: Principles & Practice|Intro to Combinatorial Optimization\n",
      "\n",
      "Sinapova, Dima  - Intro to Computational Geometry|The Claims of Fiction\n",
      "\n",
      "Henri Y. Gillet  - pompeii: art, architecture, & archaeology in the lost city|Quantum Mechanics\n",
      "\n",
      "Tafti, Ali  - topics in game-theoretic artificial intelligence|Advanced Programming for Digital Art & Literature\n",
      "\n",
      "Troy, Patrick  - the design and analysis of trading agents|Introductory Compiler Construction\n",
      "\n",
      "Aireza Mojab  - Intro. to Power Engineering|Individual Independent Study\n",
      "\n",
      "Furman, Alex  - probabilistic methods in computer science|Art After India\n",
      "\n",
      "Leilah Lyons  - writing & speaking german i|Intro. to Sceintific Computing|Reinventions of Life: Aesthetics, Biopolitics, and the Avant-Gardes|Internet and Web Algorithms\n",
      "\n",
      "piotr.gmytrasiewicz  - topics in ecology & evolutionary biology|Literary Communities|Computational Methodz for Biology|Fundamentals of Computer Systems\n",
      "\n",
      "Steven Sauerwald  - American Poetry II: Modernism|Pattern Recognition & Machine Learning|Computer Systems Security: Principles and Practice|Writing and Speaking Chinese I\n",
      "\n",
      "Mitchell Theys  - virtual citizens or subjects? the global battle over governing your internet|Cinematic Coding and Narrativity|Inventing the Past: Amulets, Heirlooms, Monuments, Landscapes\n",
      "\n",
      "Hormoz Bassiri  - Heat and Mass Transfer\n",
      "\n",
      "Philip S Yu  - altered states|Applied Artifical Intelligence\n",
      "\n",
      "Lu, Hui  - crossing the consumer chasm by design|Temporalities\n",
      "\n",
      "Bhaskar DasGupta  - cultures & countercultures: the american novel after world war ii\n",
      "\n",
      "Ning Jin  - Hilbert Spaces & Their Applications|Advanced Algorithms Seminar\n",
      "\n",
      "Wenjing Rao  - collections & visual knowledge in early modern europe|Independent Study in 2D Game Engines|Combinatorial Topology|Touring the Empire: Travel Literature and the Idea of America\n",
      "\n",
      "Metlushko, Vitali  - women's voices in medieval literature|Stem Cell Engineering|Mathematical Statistics|Melville, Conrad, and the Sea\n",
      "\n",
      "Mitch Theys  - Foundationz of Electromagnetism and Modern Physics|Distributed Computing through Combinatorial Topology\n",
      "\n",
      "Westland, Chris  - Probabilistic Graphical Models|Victorian Inequality|Melville, Conrad, & the Sea|Narrative and Immersion|Projects in Engineering Design|Introductory Calculuz, Part II\n",
      "\n",
      "A. Prasad Sistla  - pompeii: art, architecture, and archaeology in the lost city|Molecular Genetics|Intro. to Computer Graphics|El Greco and Velazquez|Algorithmic Foundations of Computational Biology\n",
      "\n",
      "Soheili, Negar  - shakespeare|Biophysical and Bioinorganic Chemistry\n",
      "\n",
      "Yu, Philip  - Computers and Human Values\n",
      "\n",
      "Krishna Reddy  - Reinventions of Life: Aesthetics, Biopolitics, and the Avant-Gardes|Altered Statez|Quantum Mechanics\n",
      "\n",
      "Ana Cui  - the postcolonial & the postmodern|Computational Theory of Molecular Evolution\n",
      "\n",
      "Ajay Kshemkalyani  - Introductory Calculus, Part I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('class.txt') as csvfile:\n",
    "    for row in csvfile:\n",
    "          print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Split the rows into logical strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "First we have to split the name of the professor from the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kshemkalyani, Ajay parallel computation: models, algorithms, limits|Filtering and Prediction of Hidden Markov Models\n"
     ]
    }
   ],
   "source": [
    "def separate_name_from_classes(row):\n",
    "    return row.split(\"  - \")\n",
    "\n",
    "row = \"Kshemkalyani, Ajay  - parallel computation: models, algorithms, limits|Filtering and Prediction of Hidden Markov Models\"\n",
    "name, classes = separate_name_from_classes(row)\n",
    "print(name, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Then we have to separate the classes into a list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parallel computation: models, algorithms, limits', 'Filtering and Prediction of Hidden Markov Models']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def separate_classes(classes):\n",
    "    return re.split(\"\\|\", classes)\n",
    "\n",
    "classes_list = separate_classes(classes)\n",
    "print(classes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Now we have the logic to split each row into logical strings that we can later clean up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II - Handling Different Name Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this CSV, names seem to be in the following formats:\n",
    "    1. John Doe\n",
    "    2. John H. Doe\n",
    "    3. John H Doe\n",
    "    4. Doe, John\n",
    "    5. Doe, H. John\n",
    "    6. Doe, H John\n",
    "    7. Doe\n",
    "    8. john.doe\n",
    "    \n",
    "In this case it would be very simple to create regular expressions to handle the various name formats into one single format. I'll use the following universal format for testing:\n",
    " - John Doe (ignoring middle initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Create regular expressions and get the last name only (only last name required for this assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_formats = ['John Doe', 'John H. Doe', 'John H Doe', 'Doe, John', 'Doe, H. John', 'Doe, H John']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n"
     ]
    }
   ],
   "source": [
    "# John Doe\n",
    "reg1 = re.match(r\"(?P<first_name>\\w+) (?P<last_name>\\w+)\", name_formats[0])\n",
    "print(\"{0} {1}\".format(reg1.group('first_name'), reg1.group('last_name'))) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n",
      "John Doe\n"
     ]
    }
   ],
   "source": [
    "# John H. Doe and John H Doe\n",
    "pattern = re.compile(r\"(?P<first_name>\\w+) (?P<middle_initial>)(\\w+.|\\w+) (?P<last_name>\\w+)\")\n",
    "reg2 = re.match(pattern, name_formats[1])\n",
    "reg3 = re.match(pattern, name_formats[2])\n",
    "print(\"{0} {1}\".format(reg2.group('first_name'), reg2.group('last_name'))) # success!\n",
    "print(\"{0} {1}\".format(reg3.group('first_name'), reg3.group('last_name'))) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n"
     ]
    }
   ],
   "source": [
    "# Doe, John\n",
    "reg4 = re.match(r\"(?P<last_name>\\w+), (?P<first_name>\\w+)\", name_formats[3])\n",
    "print(\"{0} {1}\".format(reg4.group('first_name'), reg4.group('last_name'))) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n",
      "John Doe\n"
     ]
    }
   ],
   "source": [
    "# Doe, H. John and Doe, H John\n",
    "pattern = re.compile(r\"(?P<last_name>\\w+), (?P<middle_initial>)(\\w+.|\\w+) (?P<first_name>\\w+)\")\n",
    "reg5 = re.match(pattern, name_formats[4])\n",
    "reg6 = re.match(pattern, name_formats[5])\n",
    "print(\"{0} {1}\".format(reg5.group('first_name'), reg5.group('last_name'))) # success!\n",
    "print(\"{0} {1}\".format(reg6.group('first_name'), reg6.group('last_name'))) # success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Now combine the multiple regular expressions into 2 regular expressions with the following format:\n",
    " - John Doe\n",
    " - Doe, John\n",
    " \n",
    " For example, any name with the format \"first m.i. last\" should be parsed using one regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doe\n",
      "Doe\n",
      "Doe\n"
     ]
    }
   ],
   "source": [
    "# <firstname> <m.i.>(optional) <lastname>\n",
    "pattern = re.compile(r\"(?P<first_name>\\w+)( (?P<middle_initial>)(\\w+.|\\w+) | )(?P<last_name>\\w+)\")\n",
    "reg1 = re.match(pattern, name_formats[0])\n",
    "reg2 = re.match(pattern, name_formats[1])\n",
    "reg3 = re.match(pattern, name_formats[2])\n",
    "print(reg1.group('last_name'))\n",
    "print(reg2.group('last_name'))\n",
    "print(reg3.group('last_name')) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doe\n",
      "Doe\n",
      "Doe\n"
     ]
    }
   ],
   "source": [
    "# <lastname>, <m.i>(optional) or <m.i>.(optional) <firstname>\n",
    "pattern = re.compile(r\"(?P<last_name>\\w+),( (?P<middle_initial>)(\\w+.|\\w+) | )(?P<first_name>\\w+)\")\n",
    "reg4 = re.match(pattern, name_formats[3])\n",
    "reg5 = re.match(pattern, name_formats[4])\n",
    "reg6 = re.match(pattern, name_formats[5])\n",
    "print(reg4.group('last_name'))\n",
    "print(reg5.group('last_name'))\n",
    "print(reg6.group('last_name')) # success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3:\n",
    "Let's combile the two into one easy to use function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last_name': 'Doe', 'first_name': 'John'}\n",
      "{'last_name': 'Doe', 'middle_initial': 'H', 'first_name': 'John'}\n",
      "{'last_name': 'Doe', 'middle_initial': 'H', 'first_name': 'John'}\n",
      "{'last_name': 'Doe', 'first_name': 'John'}\n",
      "{'last_name': 'Doe', 'middle_initial': 'H', 'first_name': 'John'}\n",
      "{'last_name': 'Doe', 'middle_initial': 'H', 'first_name': 'John'}\n",
      "{'last_name': 'Doe'}\n",
      "{'last_name': 'Doe'}\n",
      "{'last_name': 'Doe', 'middle_initial': 'Homer', 'first_name': 'J'}\n",
      "{'last_name': 'Doe', 'first_name': 'John'}\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(word):\n",
    "    remove_punc_pattern = r'\\w*'\n",
    "    return re.match(remove_punc_pattern, word).group(0)\n",
    "\n",
    "def get_name(unknown_name_format):\n",
    "    \n",
    "    name_dict = {}\n",
    "    pattern = None\n",
    "    \n",
    "    # format is firstname.lastname\n",
    "    if len(unknown_name_format.split('.')) == 2 and len(unknown_name_format.split(' ')) == 1:\n",
    "        first_name, last_name = unknown_name_format.split('.')\n",
    "        return {'last_name':last_name.capitalize(), 'first_name':first_name.capitalize()}\n",
    "        \n",
    "    # one word in name\n",
    "    elif len(unknown_name_format.split(' ')) == 1:\n",
    "        return {'last_name':remove_punctuation(unknown_name_format).capitalize()}\n",
    "    \n",
    "    # if comma after first word\n",
    "    elif unknown_name_format.split(\" \")[0][-1] == ',':\n",
    "        pattern = re.compile(r\"(?P<last_name>\\w+,)\\s(((?P<middle_initial>\\w+.|\\w+)\\s)|\\b)((?P<first_name>\\w+))|\\b\")\n",
    "        \n",
    "    else:\n",
    "        pattern = re.compile(r\"(?P<first_name>\\w+(.|\\b))\\s*(((?P<middle_initial>\\w+|\\w+.)\\s)|\\b)(?P<last_name>\\w*)\")\n",
    "    \n",
    "    reg = re.match(pattern, unknown_name_format)\n",
    "\n",
    "    if reg.group('first_name'):\n",
    "        name_dict['first_name'] = remove_punctuation(reg.group('first_name')).capitalize()\n",
    "\n",
    "    if reg.group('middle_initial'):\n",
    "        name_dict['middle_initial'] = remove_punctuation(reg.group('middle_initial')).capitalize()\n",
    "\n",
    "    if reg.group('last_name'):\n",
    "        name_dict['last_name'] = remove_punctuation(reg.group('last_name')).capitalize()\n",
    "    \n",
    "    return name_dict;\n",
    "\n",
    "print(get_name(\"John Doe\"))\n",
    "print(get_name(\"John H. Doe\"))\n",
    "print(get_name(\"John H Doe\"))\n",
    "print(get_name(\"Doe, John\"))\n",
    "print(get_name(\"Doe, H John\"))\n",
    "print(get_name(\"Doe, H. John\"))\n",
    "print(get_name(\"Doe\"))\n",
    "print(get_name(\"Doe,\"))\n",
    "print(get_name(\"J. Homer Doe\"))\n",
    "print(get_name(\"john.doe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Now we have the proper regular expressions to handle the various name formats that exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III - Clean up the classes\n",
    "Now that we've cleaned up the names, let's clean up the classes. By looking at the classes you'll notice that there are the following issues:\n",
    "    1. Identical courses with mismatched spelling\n",
    "    2. Incorrect spelling in some cases\n",
    "    3. Incorrect caps\n",
    "    4. More? (look at the data more closely for more possible issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "We must find how many classes are the same. The issue is that some courses are the same but have subtle differences - i.e. computer vizion vs. computer vision and intro to computer science vs. introduction to computer science. Let's experiment and see how many classes are similar to eachtoher by playing with the Edit Distance algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer vizion', 'computer graphics', 'Computer vizion', 'computer graphics', 'computer vision', 'computer graphicz']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'computer graphics': True, 'computer vizion': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def edit_distance(word1, word2):\n",
    "    \"\"\"This algorithm involves looping through word1 and converting it to word2 letter by letter,\n",
    "    replacing/deleting/inserting each character that doesn't match exactly.\"\"\"\n",
    "    \n",
    "    if len(word1) == 0:\n",
    "        return 0\n",
    "    if len(word2) == 0:\n",
    "        return 0\n",
    "    \n",
    "    word1 = list(word1.lower())\n",
    "    word2 = list(word2.lower())\n",
    "    \n",
    "    distance = 0\n",
    "    \n",
    "    # find the shortest word\n",
    "    shortest_word_len = 0\n",
    "    if len(word1) < len(word2):\n",
    "        shortest_word_len = len(word1)\n",
    "    else:\n",
    "        shortest_word_len = len(word2)\n",
    "        \n",
    "    # find the longest word\n",
    "    longest_word_len = 0\n",
    "    if len(word1) > len(word2):\n",
    "        longest_word_len = len(word1)\n",
    "    else:\n",
    "        longest_word_len = len(word2)\n",
    "    \n",
    "    # loop for the duration of the shortest word\n",
    "    # ensures there is no insertion at this point\n",
    "    # strictly deletion and replacing\n",
    "#     print(\"\".join(word1), \"\".join(word2))\n",
    "    for i in range(0, shortest_word_len):\n",
    "        \n",
    "        # letters are the same\n",
    "        if word1[i] == word2[i]:\n",
    "            pass\n",
    "        \n",
    "        # letters are not the same\n",
    "        elif word1[i] != word2[i]:\n",
    "            word1[i] = word2[i]\n",
    "#             print(\"\".join(word1), \"\".join(word2))\n",
    "            distance += 1\n",
    "    \n",
    "    # loop through any remaining characters\n",
    "    # that are empty strings and replace / delete them\n",
    "    for i in range(shortest_word_len, longest_word_len):\n",
    "        # if word2 no longer has letters, than word1 is longer\n",
    "        # deletion is needed\n",
    "        if i >= shortest_word_len and len(word1) > len(word2):\n",
    "            word1.pop()\n",
    "            distance += 1\n",
    "#             print(\"\".join(word1), \"\".join(word2))\n",
    "            continue\n",
    "        \n",
    "        # if letters don't exist in word1 that are in word2\n",
    "        # insertion is needed\n",
    "        try:\n",
    "            word1.append(word2[i])\n",
    "#             print(\"\".join(word1), \"\".join(word2))\n",
    "            distance += 1\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return distance\n",
    "\n",
    "assert edit_distance('kitten', 'sitting') is 3\n",
    "\n",
    "test_classes = [['computer vizion', 'computer graphics'],\n",
    "               ['Computer vizion', 'computer graphics'],\n",
    "               ['computer vision', 'computer graphicz']]\n",
    "test_classes = [item for sublist in test_classes for item in sublist]\n",
    "\n",
    "# loop through each class and see if they are similar\n",
    "def entity_resolution_between_list(test_classes, distance_constant=5):\n",
    "    distinct_classes = {}\n",
    "    for i in range(0, len(test_classes)):\n",
    "        a_class = test_classes[i].lower()\n",
    "        for j in range(i, len(test_classes)):\n",
    "            if i == j:\n",
    "                pass\n",
    "            elif edit_distance(a_class, test_classes[j].lower()) <= distance_constant:\n",
    "                try:\n",
    "                    distinct_classes[a_class] = True\n",
    "                except KeyError:\n",
    "                    distinct_classes.append({a_class, True})\n",
    "    return distinct_classes;\n",
    "                    \n",
    "entity_resolution_between_list(test_classes, 1)\n",
    "\n",
    "# now we can corrent spelling as needed that getting the distinct classes is possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Let's begin with ensuring each sentence has proper spelling. This is especially difficult because we have to account for special cases such as roman numerals and abbreviations (i.e. intro).\n",
    "\n",
    "For the first step of spelling, I will use WordNet, the python library \"enchant\", and an online service called DictService to discover when a word is mispelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# to be a dictionary of words mapped with the value true\n",
    "wordnet_dict = {}\n",
    "\n",
    "def load_wordnet_into_mem(filename):\n",
    "    \"\"\"Loads a wordnet csv into a dictionary for later use. The load is only completed once so each lookup can be done\n",
    "    constant time.\"\"\"\n",
    "\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for word in reader:\n",
    "            wordnet_dict[word[0]] = True\n",
    "            \n",
    "load_wordnet_into_mem('WordNet.csv')\n",
    "print(\"done\")\n",
    "\n",
    "# let's give it a test\n",
    "print(wordnet_dict['3d'])\n",
    "print(wordnet_dict['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def dict_service_has_word(word, strategy='exact'):\n",
    "    \n",
    "    url = 'http://services.aonaware.com/DictService/DictService.asmx/Match?word={0}&strategy={1}'.format(word, strategy)\n",
    "#     print(url)\n",
    "    results = urllib.request.urlopen(url).read()\n",
    "    root = ET.fromstring(results)\n",
    "    for child in root:\n",
    "        for found_word in child:\n",
    "            if found_word.text == word:\n",
    "                return True\n",
    "    return False\n",
    "    \n",
    "print(dict_service_has_word('asdfa'))\n",
    "print(dict_service_has_word('the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelz models\n",
      "['models', 'model', \"model's\", 'modals', 'motels', 'modules', 'modeler', 'modeled', 'modelers', 'modal', \"modal's\", 'modes', 'modulus', 'motel', \"motel's\", 'module', \"mode's\", 'modulo', 'modems', 'morels', 'yodels', 'medals', \"module's\", \"modeler's\", \"Godel's\", \"modem's\", \"morel's\", \"yodel's\", \"medal's\"]\n",
      "limitz limits\n",
      "['limits', 'limit', \"limit's\", 'Nimitz', 'limiter', 'limited', 'limiters', \"limiter's\"]\n",
      "Modelz Models\n",
      "['Models', 'Model', \"Model's\", 'Modals', 'Motels', 'Modules', 'Modeler', 'Modeled', 'Modelers', 'Modal', \"Modal's\", 'Modes', 'Modulus', 'Motel', \"Motel's\", 'Module', \"Mode's\", 'Modulo', 'Modems', 'Morels', 'Yodels', 'Medals', \"Module's\", \"Modeler's\", \"Godel's\", \"Modem's\", \"Morel's\", \"Yodel's\", \"Medal's\"]\n",
      "vizion vision\n",
      "['viz ion', 'viz-ion', 'vision', 'Zion', 'vicing', 'vising', 'Vinson', 'vizier', 'Villon', 'Vivian', 'vino', 'Verizon', 'visaing', 'voicing', 'viz', 'vixen', 'Viking', 'sizing', 'viking', 'violin', 'virgin', 'vain', 'vein', 'wising', 'venison', 'scion', 'visor', \"vino's\"]\n",
      "Biomaterials\n",
      "['Bio materials', 'Bio-materials', 'Materials', \"Material's\", 'Immaterial', 'Material', \"Materiel's\", 'Bilateral', 'Imperials', 'Immaterially', \"Imperial's\"]\n",
      "chineze Chinese\n",
      "['Chinese', 'chines', \"chine's\", 'chine', 'chins', 'chintz', 'Chance', \"Chin's\", 'chance', \"chin's\", 'chinos', 'shines', 'chintzy', \"China's\", \"china's\", \"chino's\", \"shine's\", 'shiners', 'chintzier', 'Chin', 'chains', 'chin', 'Inez', \"Ch'in's\", 'China', \"Chinese's\", \"chain's\", 'china', 'chino', 'shine', \"Chaney's\", \"Cheney's\", \"shiner's\", \"Ch'in\"]\n",
      "Telefantasy Telephonist\n",
      "['Elephants', 'Talents', \"Elephant's\", \"Levant's\", \"Talent's\", 'Telephotos', 'Telephonist', \"Telephony's\", \"Delft's\", 'Telephones', 'Deviants', \"Telephone's\", \"Telephoto's\", \"Deviant's\"]\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "import re\n",
    "\n",
    "# TODO implement exact match using - http://services.aonaware.com/DictService/DictService.asmx\n",
    "# return immediately if there is an exact match\n",
    "# TODO account for cases where a word is supposed to have a hyphen or a space\n",
    "# remove the hyphen or space and see if the word is still the same, be careful with cases \n",
    "# TODO load wordnet csv into memory as a dict for use with words such as 2d and 3d\n",
    "\n",
    "def soundex_distance(word):\n",
    "    # source: https://en.wikipedia.org/wiki/Soundex\n",
    "    \n",
    "    consonants = ['b', 'f', 'p', 'v', 'c', 'g', 'j', 'k', 'q', 's', 'x', 'z', 'd', 't', 'l', 'm', 'n', 'r']\n",
    "    \n",
    "    # corresponds to the soudex mapping\n",
    "    mapping = {\n",
    "        'b':1, 'f':1, 'p':1, 'v':1,\n",
    "        'c':2, 'g':2, 'j':2, 'k':2, 'q':2, 's':2, 'x':2, 'z':2,\n",
    "        'd':3, 't':3,\n",
    "        'l':4,\n",
    "        'm':5, 'n':5,\n",
    "        'r':6\n",
    "    }\n",
    "    \n",
    "    # Save the first letter. Remove all occurrences of 'h' and 'w' except first letter\n",
    "    first_letter = word[0]\n",
    "    word = word[0] + re.sub(r'[HhWw]', '', word[1:])\n",
    "    \n",
    "    # Replace all consonants (include the first letter) with digits as in [2.] above\n",
    "    word = list(word)\n",
    "    for i, letter in enumerate(word):\n",
    "        if letter.lower() in consonants:\n",
    "            word[i] = str(mapping[letter.lower()])\n",
    "            \n",
    "    word = ''.join(word)\n",
    "    \n",
    "    # Replace all adjacent same digits with one digit.\n",
    "    temp_word = word[0]\n",
    "    i = 1\n",
    "    while i < len(word):  \n",
    "        if word[i] != word[i-1]:\n",
    "            temp_word += word[i]\n",
    "        i+=1\n",
    "    \n",
    "    word = temp_word\n",
    "    \n",
    "    # Remove all occurrences of a, e, i, o, u, y except first letter.\n",
    "    word = word[0] + re.sub(r'[aeiouyhw]', '', word[1:])\n",
    "    \n",
    "    # If first symbol is a digit replace it with letter saved on step 1.\n",
    "    word = re.sub(r'[0-9]', first_letter, word[0]) + word[1:]\n",
    "    \n",
    "    # Append 3 zeros if result contains less than 3 digits. Remove all except first letter and 3 digits after it (This step same as [4.] in explanation above).\n",
    "    while len(word) < 4:\n",
    "        word += '0'\n",
    "        \n",
    "    return word[:4]\n",
    "\n",
    "def correct_sentence_spelling(sentence):\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    corrected_sentence = []\n",
    "    \n",
    "    for word_with_special_chars in sentence.split(\" \"):\n",
    "        word = ''\n",
    "        reg = None\n",
    "        \n",
    "        # replace any '&' with 'and'\n",
    "        if word_with_special_chars == '&':\n",
    "            word = 'and'\n",
    "            continue\n",
    "            \n",
    "        reg = re.match(r\"(?P<pre>[.,!?;:']*)(?P<word>[\\w&]*)(?P<post>[.,!?;:']*)\", word_with_special_chars) # group words and punctuation\n",
    "        alpha_numeric_word = reg.group('word')\n",
    "        \n",
    "        #save punctuation before and after the word\n",
    "        pre = reg.group('pre')\n",
    "        post = reg.group('post')\n",
    "        \n",
    "        if d.check(alpha_numeric_word) == False: # word isn't spelled correctly\n",
    "            \n",
    "            # find first closest suggested word\n",
    "            for suggested_word in d.suggest(alpha_numeric_word):\n",
    "                \n",
    "#                 print(alpha_numeric_word, suggested_word, d.suggest(alpha_numeric_word))\n",
    "                word = ''\n",
    "                \n",
    "                # both words are the same length and the soundex is the same\n",
    "                if len(alpha_numeric_word) == len(suggested_word) \\\n",
    "                and soundex_distance(alpha_numeric_word.lower()) == soundex_distance(suggested_word.lower()):\n",
    "#                     print(alpha_numeric_word, suggested_word)\n",
    "                    test = input(alpha_numeric_word + \" \" + suggested_word)\n",
    "                    print(d.suggest(alpha_numeric_word))\n",
    "                    word = suggested_word\n",
    "                    break\n",
    "                    \n",
    "                # both words are not the same length but they sounds the same and are separted by a space or hyphen \"-\"\n",
    "                # i'm guessing this method is more unstable\n",
    "#                 elif soundex_distance(alpha_numeric_word) == soundex_distance(suggested_word.replace(\"-\", \"\").replace(\" \", \"\")):\n",
    "# #                     test = input(alpha_numeric_word + \" \" + suggested_word)\n",
    "#                     print(alpha_numeric_word, suggested_word)\n",
    "#                     word = suggested_word\n",
    "#                     break\n",
    "            # word still hasn't been correct yet\n",
    "            # let's ignore it for now\n",
    "            if len(word) == 0:\n",
    "                test = input(alpha_numeric_word)\n",
    "                print(d.suggest(alpha_numeric_word))\n",
    "                word = alpha_numeric_word\n",
    "        else:\n",
    "            word = alpha_numeric_word\n",
    "        \n",
    "        word = word_with_special_chars.replace(word_with_special_chars, word)\n",
    "        corrected_sentence.append(pre+word+post)\n",
    "    return \" \".join(corrected_sentence)\n",
    "\n",
    "soundex_test_list = ['Robert', 'Rupert', 'Ashcraft', 'Ashcroft', 'Tymczak', 'Pfister', 'Vision', 'Vizion']\n",
    "for word in soundex_test_list:\n",
    "#     print(soundex_distance(word))\n",
    "    pass\n",
    "\n",
    "example_class_list = ['parallel computation: modelz, algorithms, limitz',\n",
    "                      'Filtering and Prediction of Hidden Markov Modelz', 'computer vizion part ii', \n",
    "                      \"Descartes' World\", 'American Fiction & Mass Culture\\n', 'Biomaterials', 'chineze', 'Telefantasy']\n",
    "\n",
    "for a_class_dirty in example_class_list:\n",
    "    a_class_clean = correct_sentence_spelling(a_class_dirty)\n",
    "#     print(a_class_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "Now that we've ensured the spelling is correct for the most part, we need to find a way to capitalize each word appropriately. We should take the following rules into account:\n",
    "    - First word of a sentence is capitalized\n",
    "    - Last word of a sentence is capitalized\n",
    "    - The following words will not be capitalized: a, an, the, at, by, for, in, of, on, to, up, and, as, but, or. \n",
    "\n",
    "[Word Source](http://grammar.yourdictionary.com/capitalization/rules-for-capitalization-in-titles.html#QKr4elbmMtimKfJz.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pompeii's Fall 101: Art, Architecture, & Archaeology in the Lost City\n",
      "Descartes' World World\n",
      "Biomaterials\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def fix_sentence_case(sentence):\n",
    "    \n",
    "    lower_case_words = ['a', 'an', 'the', 'at', 'by', 'for', 'in', 'of', 'on', 'to', 'up', 'and', 'as', 'but', 'or']\n",
    "    \n",
    "    old_sentence_list = sentence.split(\" \")\n",
    "    \n",
    "    first_word = old_sentence_list[0].capitalize()\n",
    "    \n",
    "    if len(old_sentence_list) == 1:\n",
    "        return first_word\n",
    "    \n",
    "    last_word = old_sentence_list[-1].capitalize()\n",
    "    \n",
    "    new_sentence = []\n",
    "    new_sentence.append(first_word)\n",
    "    \n",
    "    old_sentence_list.pop(0) # remove first word, it's been capitalized and saved\n",
    "    \n",
    "    if len(old_sentence_list) > 1:\n",
    "        old_sentence_list.pop(len(old_sentence_list)-1) # remove the last word, it's been capitalized and saved\n",
    "\n",
    "    for word in old_sentence_list:\n",
    "            \n",
    "        reg_ex = re.match(r\"(\\w+)\", word) # strip special characters i.e. ':' or ','\n",
    "        \n",
    "        # test that the word is alpha numeric\n",
    "        if reg_ex is not None and reg_ex.group(0) not in lower_case_words:\n",
    "            alpha_numeric_word = reg_ex.group(0)\n",
    "            capitalized_word = alpha_numeric_word.capitalize()\n",
    "            word = word.replace(alpha_numeric_word, capitalized_word) # maintain special chars i.e. 'Test:' or 'yes,'\n",
    "            new_sentence.append(word)\n",
    "            \n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "\n",
    "    new_sentence.append(last_word)\n",
    "    return \" \".join(new_sentence)\n",
    "\n",
    "sentences = ['pompeii\\'s fall 101: art, architecture, & archaeology in the lost city', 'Descartes\\' World', 'Biomaterials']\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(fix_sentence_case(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV - Combining the Pieces\n",
    "Now that we've built a series of functions that are workable with our given text file, we can now combine them to create a \"clean copy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Take a list of classes and append them with a \"|\" as in the original text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sentence|this is also a sentence|this is another sentence\n"
     ]
    }
   ],
   "source": [
    "def join_sentences_with_pipe(sentences):\n",
    "    return '|'.join(sentences)\n",
    "\n",
    "sentences = ['this is a sentence', 'this is also a sentence', 'this is another sentence']\n",
    "print(join_sentences_with_pipe(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Now that we've merges the classes, we can now combine the classes with the professor name. In this example, I will use the last name of the professor because it's conveniately unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doe  - Biology|Science of Mind\n"
     ]
    }
   ],
   "source": [
    "def join_name_with_classes(name, classes):\n",
    "    return name + '  - ' + classes\n",
    "\n",
    "classes = 'Biology|Science of Mind'\n",
    "name = 'Doe'\n",
    "print(join_name_with_classes(name, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "Now that we've cleaned the data, let's combine the data into a dictionary where the professor name is the key and the courses he/she teaches is the value.\n",
    "\n",
    "Note: In this case, we must assume that there are duplicate rows in the CSV. A dictionary will handle this case well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Doe': ['The science of mind', \"Let's Embrace Fear\"]}\n",
      "{'Doe': ['The science of mind', \"Let's Embrace Fear\", 'Living life fully']}\n",
      "{'Doe': ['The science of mind', \"Let's Embrace Fear\", 'Living life fully'], 'Smith': [\"It's just the way it is\"]}\n"
     ]
    }
   ],
   "source": [
    "def build_dictionary(dictionary, name, classes):\n",
    "    try:\n",
    "        for a_class in classes:\n",
    "            dictionary[name].append(a_class)\n",
    "    except KeyError:\n",
    "        dictionary[name] = classes\n",
    "    return dictionary\n",
    "\n",
    "temp_dict = {}\n",
    "name = 'Doe'\n",
    "classes = ['The science of mind', 'Let\\'s Embrace Fear']\n",
    "\n",
    "print(build_dictionary(temp_dict, name, classes))\n",
    "\n",
    "name = 'Doe'\n",
    "classes = ['Living life fully']\n",
    "\n",
    "print(build_dictionary(temp_dict, name, classes))\n",
    "\n",
    "name = 'Smith'\n",
    "classes = ['It\\'s just the way it is']\n",
    "\n",
    "print(build_dictionary(temp_dict, name, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "For the final step, let's combine the functions we created throughout this tutorial to generate a document that is properly capitalized, and spelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correct_sentence_spelling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-f0ab7f89fd78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mclasses_list_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma_class_dirty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses_list_dirty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0ma_class_dirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_sentence_spelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_class_dirty\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0ma_class_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_sentence_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_class_dirty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mclasses_list_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_class_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'correct_sentence_spelling' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import enchant\n",
    "import re\n",
    "\n",
    "cleaned_data_dict = {}\n",
    "with open('class.txt') as csvfile:\n",
    "    csv_data_dict = {}\n",
    "    for row_dirty in csvfile:\n",
    "        name_dirty, classes_dirty = separate_name_from_classes(row_dirty)\n",
    "        classes_list_dirty = separate_classes(classes_dirty)\n",
    "        name_clean = get_name(name_dirty)['last_name']\n",
    "        classes_list_clean = []\n",
    "        for a_class_dirty in classes_list_dirty:\n",
    "            a_class_dirty = correct_sentence_spelling(a_class_dirty) # TODO\n",
    "            a_class_clean = fix_sentence_case(a_class_dirty)\n",
    "            classes_list_clean.append(a_class_clean)\n",
    "        cleaned_data_dict = build_dictionary(cleaned_data_dict, name_clean, classes_list_clean)\n",
    "#         classes_clean = join_sentences_with_pipe(classes_list_clean)\n",
    "#         row_clean = join_name_with_classes(name_clean, classes_clean)\n",
    "#         print(row_clean)\n",
    "print(cleaned_data_dict)\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
